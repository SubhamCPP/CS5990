#-------------------------------------------------------------------------
# AUTHOR: Subham Panda
# FILENAME: bagging_random_forest
# SPECIFICATION: Implementing and comparing performance of bagging and random forest algorithms.
# FOR: CS 5990- Assignment #4
# TIME SPENT: 2 hours
#-----------------------------------------------------------*/

#importing some Python libraries
from sklearn import tree
from sklearn.utils import resample
from sklearn.ensemble import RandomForestClassifier
import pandas as pd
from sklearn.preprocessing import LabelEncoder

dbTraining = []
dbTest = []
X_training = []
y_training = []
classVotes = [] #this array will be used to count the votes of each classifier

#reading the training data from a csv file and populate dbTraining
#--> add your Python code here
dbTraining = pd.read_csv('optdigits.tra', sep=',', header=None)
#reading the test data from a csv file and populate dbTest
#--> add your Python code here
dbTest = pd.read_csv('optdigits.tes', sep=',', header=None).to_numpy()
#inititalizing the class votes for each test sample. Example: classVotes.append([0,0,0,0,0,0,0,0,0,0])
#--> add your Python code here
n_dbTest = len(dbTest)
for i in range(n_dbTest):
    classVotes.append([0,0,0,0,0,0,0,0,0,0])

print("Started my base and ensemble classifier ...")

for k in range(20): #we will create 20 bootstrap samples here (k = 20). One classifier will be created for each bootstrap sample

    bootstrapSample = resample(dbTraining, n_samples=len(dbTraining), replace=True)

  #populate the values of X_training and y_training by using the bootstrapSample
  #--> add your Python code here
    X_training = bootstrapSample.iloc[:,:-1]
    y_training = bootstrapSample.iloc[:,-1]
  #fitting the decision tree to the data
    clf = tree.DecisionTreeClassifier(criterion = 'entropy', max_depth=None) #we will use a single decision tree without pruning it
    clf = clf.fit(X_training, y_training)
    #print(clf)
    single_TestPrediction = 0
    for i, testSample in enumerate(dbTest):
        X_dbTest = [testSample[:-1]]
        prediction = clf.predict(X_dbTest)[0]
        classVotes[i][prediction] += 1
        
        if k == 0: #for only the first base classifier,
        #compare the prediction with the true label of the test sample here to start calculating its accuracy
         #--> add your Python code here
            y_dbTest = testSample[-1]
            if prediction == y_dbTest:
                single_TestPrediction += 1
                
    if k == 0: #for only the first base classifier, print its accuracy here
    #--> add your Python code here
        accuracy = single_TestPrediction/len(dbTest)
        print("Finished my base classifier (fast but relatively low accuracy) ...")
        print("My base classifier accuracy: " + str(accuracy))
        print("")
        
    #now, compare the final ensemble prediction (majority vote in classVotes) 
    #for each test sample with the ground truth label to calculate the accuracy of
    #the ensemble classifier (all base classifiers together)
      #--> add your Python code here
    all_TestPredictions = 0
    for i, testSample in enumerate(dbTest):
        #X_dbTest = [testSample[:-1]]
        y_dbTest = testSample[-1]
        max_classVotes = classVotes[i].index(max(classVotes[i]))
        if y_dbTest == max_classVotes:
            all_TestPredictions += 1
    accuracy = all_TestPredictions/len(dbTest)
    #print('Check', max_classVotes)
    #printing the ensemble accuracy here
    print("Finished my ensemble classifier (slow but higher accuracy) ...")
    print("My ensemble accuracy: " + str(accuracy))
    print("")

    print("Started Random Forest algorithm ...")

    #Create a Random Forest Classifier
    clf=RandomForestClassifier(n_estimators=20) #this is the number of decision trees that will be generated by Random Forest. The sample of the ensemble method used before

    #Fit Random Forest to the training data
    clf.fit(X_training,y_training)

    #make the Random Forest prediction for each test sample. Example: class_predicted_rf = clf.predict([[3, 1, 2, 1, ...]]
    #--> add your Python code here
    n_dbTestRF = 0
    for i, testSample in enumerate(dbTest):
        X_dbTest = [testSample[:-1]]
        y_dbTest = testSample[-1]
        prediction = clf.predict(X_dbTest)[0]
        if y_dbTest == prediction:
            n_dbTestRF += 1
    #compare the Random Forest prediction for each test sample with the ground truth label to calculate its accuracy
    #--> add your Python code here
    accuracy = n_dbTestRF/len(dbTest)
    #printing Random Forest accuracy here
    print("Random Forest accuracy: " + str(accuracy))

    print("Finished Random Forest algorithm (much faster and higher accuracy!) ...")
